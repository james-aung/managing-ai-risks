<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <script src="template.v2.js"></script>
  <style>
  </style>
</head>

<body>
  <style>
    body {
      font-family: Georgia, 'Times New Roman', Times, serif;
    }

    d-byline {
      background-color: #f8f9fa;
    }

    d-title {
      
    }
    
    .byline .authors-affiliations {
      grid-column: 1 / -1;
      white-space: nowrap;
      margin-bottom: 15px !important;
    }

    @media (min-width: 768px) {

      .byline .authors-affiliations {
        grid-template-columns: 1fr 1fr 1fr 1fr;
      }
      
    }

    .author-affiliation {
      margin-bottom: 5px;
    }
    
    d-byline .name {
      font-weight: 700;
    }

    d-byline .affiliation {
      font-size: 0.7rem;
    }

  
  </style>

  <d-front-matter>
    <script type="text/json">{
      "title": "Managing risks from AI in an era of rapid progress",
      "description": "Lorem ipsum",
      "authors": [
        {
          "author": "John Doe",
          "authorURL": "",
          "affiliation": "Unknown University",
          "affiliationURL": ""
        },
        {
          "author": "Jane Smith",
          "authorURL": "",
          "affiliation": "Institute for Something",
          "affiliationURL": ""
        },
        {
          "author": "Emily Johnson",
          "authorURL": "",
          "affiliation": "Someplace University",
          "affiliationURL": ""
        },
        {
          "author": "Robert Brown",
          "authorURL": "",
          "affiliation": "Place of Study",
          "affiliationURL": ""
        },
        {
          "author": "Alice Williams",
          "authorURL": "",
          "affiliation": "Fictional Institute",
          "affiliationURL": ""
        },
        {
          "author": "Michael Davis",
          "authorURL": "",
          "affiliation": "Department of Something",
          "affiliationURL": ""
        },
        {
          "author": "Sarah Wilson",
          "authorURL": "",
          "affiliation": "Place University",
          "affiliationURL": ""
        },
        {
          "author": "William Clark",
          "authorURL": "",
          "affiliation": "Research Institute",
          "affiliationURL": ""
        },
        {
          "author": "Patricia Lewis",
          "authorURL": "",
          "affiliation": "Another University",
          "affiliationURL": ""
        },
        {
          "author": "Chris Lee",
          "authorURL": "",
          "affiliation": "Study Place",
          "affiliationURL": ""
        },
        {
          "author": "Susan Scott",
          "authorURL": "",
          "affiliation": "Imaginary College",
          "affiliationURL": ""
        },
        {
          "author": "Henry Adams",
          "authorURL": "",
          "affiliation": "Tech Institute",
          "affiliationURL": ""
        },
        {
          "author": "Katie Johnson",
          "authorURL": "",
          "affiliation": "Scientific Place",
          "affiliationURL": ""
        },
        {
          "author": "Paul Miller",
          "authorURL": "",
          "affiliation": "Learning Institute",
          "affiliationURL": ""
        },
        {
          "author": "Grace Evans",
          "authorURL": "",
          "affiliation": "College of Things",
          "affiliationURL": ""
        },
        {
          "author": "Matthew Baker",
          "authorURL": "",
          "affiliation": "Place of Learning",
          "affiliationURL": ""
        },
        {
          "author": "Anna Green",
          "authorURL": "",
          "affiliation": "Education Place",
          "affiliationURL": ""
        },
        {
          "author": "Peter Hall",
          "authorURL": "",
          "affiliation": "University of Anywhere",
          "affiliationURL": ""
        },
        {
          "author": "Laura Mitchell",
          "authorURL": "",
          "affiliation": "Made-up University",
          "affiliationURL": ""
        },
        {
          "author": "Steven Allen",
          "authorURL": "",
          "affiliation": "Another Place",
          "affiliationURL": ""
        }
      ]

    }
</script>
  </d-front-matter>

  <d-title>
    <h1>Managing risks from AI in an era of rapid progress</h1>
  </d-title>


  <d-article>

    <p>Four years ago, GPT-2 could not reliably count to ten. Today, deep learning models can write software, generate
      photorealistic scenes on demand, advise on intellectual topics, and combine language and image processing to steer
      robots. As AI developers scale these systems, unforeseen abilities and behaviors emerge spontaneously, without
      explicit programming.<d-cite key="Wei2022-mz"></d-cite> Progress in AI has been swift and, to
      many, surprising.</p>
    <p>The pace of progress may surprise us again. Current deep learning systems still lack important capabilities and
      we do not know how long it will take to develop them. However, companies are engaged in a race to create AI
      systems that match or exceed human abilities across most cognitive work <d-cite
        key="DeepMind_undated-dk,OpenAI2023-bl"></d-cite>. To reach this goal, they are rapidly deploying
      more
      resources and developing new techniques. Progress in AI also enables faster progress; AI assistants are
      increasingly used to automate programming <d-cite key="Tabachnyk_undated-kq">[4]</d-cite> and
      data
      collection <d-cite key="OpenAI2023-bq,Bai2022-sv">[5,6]</d-cite> to further improve AI systems
      <d-cite key="noauthor_undated-tf">[7]</d-cite>.
    </p>
    <p>There is no fundamental reason why AI progress would slow or halt at the human level. Indeed, AI has already
      surpassed human abilities in narrow domains like protein folding and game play <d-cite
        key="Jumper2021-jy,Brown2019-pw,Campbell2002-dt">[8–10]</d-cite>. Compared to human beings, AI systems
      can act
      faster, absorb more knowledge, and communicate at a far higher bandwidth. Additionally, they can be scaled to make
      use of greater computational resources, and can be replicated by the millions.</p>
    <p>The rate of improvement is already staggering, and tech companies have the cash reserves needed to scale the
      latest training runs by multiples of 100 to 1000 <d-cite key="Alphabet2022-ro">[11]</d-cite>.
      Combined
      with the ongoing growth and automation in AI R&D, we must take seriously the possibility that generalist AI
      systems will outperform humans across many critical domains within this decade or the next.</p>
    <p>What happens then? If managed carefully and distributed fairly, highly advanced AI systems could help humanity
      cure diseases, elevate living standards, and protect our ecosystems. The opportunities AI offers are immense. But
      alongside advanced AI capabilities come large-scale risks that we are not on track to handle well. Humanity is
      pouring immense resources into making AI systems more powerful, but far less into safety and mitigating harms. For
      AI to be a boon, we need to reorient; pushing AI capabilities is not enough.</p>
    <p>We are already behind schedule for this reorientation. We must anticipate the amplification of ongoing harms, as
      well as novel risks, and prepare for the largest risks <em>well before they materialize</em>. Climate change has
      taken decades to be acknowledged and confronted; for AI, decades could be too long.</p>
    <h2 id="high-stakes-risks">High stakes risks</h2>
    <p>Advanced AI systems pose a range of large-scale risks. If carelessly-developed AI systems rapidly perform an
      increasing number of activities more cheaply and more quickly than humans, they threaten to amplify social
      injustice, undermine our professions, erode social stability, enable large-scale criminal or terrorist activities,
      and weaken our shared understanding of reality that is foundational to society. Especially in the hands of a few
      powerful actors, AI could encourage automated warfare, facilitate customized mass manipulation, unlock pervasive
      surveillance, enable and embolden totalitarian regimes, and cement or exacerbate global inequities <d-cite
        key="Hendrycks2023-zc,Weidinger2022-he">[12,13]</d-cite>.</p>
    <p>A particular set of risks comes with autonomous AI: systems that are not only highly capable but can plan, pursue
      goals, and act in the world. While current AI systems have limited autonomy, work is underway to change this
      <d-cite key="Wang2023-tq">[14]</d-cite>. For example, the non-autonomous GPT-4 model
      was quickly
      adapted to browse the web <d-cite key="openai2023chatgpt">[15]</d-cite>, design and execute
      chemistry
      experiments <d-cite key="bran2023chemcrow">[16]</d-cite>, and utilize software tools
      <d-cite key="mialon2023augmented">[17]</d-cite> including other AI models <d-cite
        key="shen2023hugginggpt">[18]</d-cite>.
    </p>
    <p>If we build highly advanced autonomous AI, we risk creating systems that autonomously pursue undesirable goals.
      Malicious actors could deliberately embed harmful objectives. Additionally, we currently don't know how to
      align AI behavior with complex values in a reliable way. Even well-meaning developer may inadvertently build AI
      systems that pursue unintended goals—especially if, in a bid to win the AI race, they neglect expensive safety testing and human oversight.</p>
    <p>If we build capable autonomous AI systems pursuing undesirable goals, we may not be able to keep them in check.
      Control of software is not a new problem; computer worms have long been able to proliferate and avoid detection
      <d-cite key="denning1989internetworm">[19]</d-cite>. However, AI is making progress in critical
      domains such as
      hacking, social manipulation, deception, and strategic planning <d-cite
        key="wang2023survey,park2023aideception">[14,20]</d-cite>. Advanced autonomous AI systems will pose
      unprecedented control challenges.
    </p>
    <p>To advance undesirable goals, autonomous AI systems could use undesirable strategies---learned from humans or
      developed independently-as a means to an end <d-cite
        key="turner2019optimal,perez2022discovering,pan2023do,hadfield2017offswitch">[21–24]</d-cite>. They could gain human trust,
      acquire
      financial resources, influence key decision-makers, and form coalitions with human actors and other AI systems. To
      avoid human intervention <d-cite key="hadfield2017offswitch">[24]</d-cite>, they could copy their
      algorithms
      across global server networks akin to computer worms. AI assistants are already co-writing a large share of
      computer code worldwide <d-cite key="dohmke2023github">[25]</d-cite>; future AI systems
      could insert
      and then exploit security vulnerabilities to control the computer systems behind our communication, media,
      banking, supply chains, militaries, or governments. In open conflicts, AI systems could threaten with or use
      autonomous or biological weapons. AI having access to such technology would merely be a continuation of existing
      trends to automate military activity, biological research, and AI development itself. If AI systems pursue such
      strategies with sufficient skill, it would be difficult for humans to intervene effectively.</p>
    <p>Finally, AI may not need to plot for influence if it is freely handed over. As autonomous AI systems increasingly
      become more cost-effective than human workers, competitive pressures will push companies, governments, and
      militaries to cede more and more control <d-cite
        key="hendrycks2023natural,chan2023harms">[26,27]</d-cite>. Like
      this, AI systems could come to control key roles in our society, without (slow and expensive) human verification
      of AI decisions and goals.</p>
    <p>Without sufficient caution, humanity may irreversibly lose control over autonomous AI systems and might even
      become marginalized or extinct.</p>
    <p>Some of the discussed AI-induced harms are already evident today and may get worse as AI systems are deployed
      more pervasively <d-cite key="bommasani2021opportunities">[28]</d-cite>; other harms show signs of
      emerging <d-cite key="park2023aideception">[20]</d-cite>. It is vital to both address ongoing
      harms and anticipate
      emerging risks. This is <em>not</em> a question of either/or. Moreover, present and emerging risks often share
      similar mechanisms, patterns, and solutions <d-cite key="brauner2023ai">[29]</d-cite>;
      investing in
      governance frameworks and AI safety will bear fruit on multiple fronts <d-cite
        key="center2023policy">[30]</d-cite>.</p>
    <h2 id="a-path-forward">A path forward</h2>
    <p>If advanced autonomous AI systems were developed today, we would not know how to make them safe, nor how to
      properly test their safety. Even if we did, most countries lack the institutions to prevent misuse and uphold safe
      practices. That does not, however, mean there is no viable path forward. To ensure a positive outcome, we can and
      must pursue research breakthroughs in AI safety and ethics, and establish effective government oversight.</p>
    <p>We need research breakthroughs to solve some of today&#39;s technical challenges in creating AI with safe and
      ethical objectives. Some challenges are not on a path to being solved by simply making AI systems more capable
      <d-cite key="perez2022discovering,mckenzie2023inverse,pan2022effects,wei2023simple,hendrycks2021unsolved,casper2023open">[22,31–35]</d-cite>. These
      include:
    </p>
    <ul>
      <li>Oversight and honesty: More capable AI systems are better able to exploit weaknesses in oversight and testing
        <d-cite key="pan2022effects,zhuang2020consequences,gao2023scaling">[32,36,37]</d-cite> – for example, by producing
        false but
        compelling output <d-cite key="casper2023open,learning2023human">[35,38]</d-cite>.
      </li>
      <li>Robustness: AI systems behave unpredictably in new situations (under distribution shift or adversarial inputs)
        <d-cite key="learning2023human,shah2022goal,ngo2022alignment">[39–41]</d-cite>.
      </li>
      <li>Interpretability: AI decision-making is opaque. So far, we can only test large models via trial and error. We
        need to learn to understand their inner workings <d-cite
          key="rauker2023transparent">[42]</d-cite>.</li>
      <li>Risk evaluations: Frontier AI systems develop unforeseen capabilities only discovered during training or even
        well after deployment <d-cite key="wei2022chain">[43]</d-cite>. With better
        evaluations, such
        capabilities could be caught earlier <d-cite key="shevlane2023model,koessler2023risk">[44,45]</d-cite>.
      </li>
      <li>Emerging challenges: More capable future systems may exhibit failure modes we have so far only seen in
        theoretical models. They might, for example, learn to feign obedience or exploit weaknesses in our safety
        objectives and shutdown mechanisms to advance a given goal <d-cite
          key="hadfield2017offswitch,ngo2022alignment">[24,41]</d-cite>.</li>
    </ul>
    <p>Given the stakes, we call on major tech companies and public funders to allocate at least one-third of their AI
      R&amp;D budget to safety and ethical use, on par with their funding for AI capabilities. Addressing these problems
      <d-cite key="hendrycks2021unsolved">[34]</d-cite>, with an eye toward powerful future systems,
      must become
      central to our field.
    </p>
    <p>We also need national institutions and international governance to enforce standards, in order to prevent
      recklessness and misuse. Many areas of technology, from pharmaceuticals, to financial systems, to nuclear energy
      show that societies both require and effectively use governance to reduce risks. However, most countries have no
      comparable governance frameworks for AI. Without them, companies and countries may seek a competitive edge by
      pushing AI capabilities to new heights while cutting corners on safety, or allowing AIs to autonomously manage
      critical societal functions with little human oversight <d-cite
        key="hendrycks2023natural">[26]</d-cite>.
      Like industry releasing waste into rivers to cut costs, they may be tempted to reap the rewards of AI development
      while leaving society to deal with the consequences.</p>
    <p>To keep up with rapid progress and avoid inflexible laws, national institutions need the authority to act
      swiftly. To address international race dynamics, they need the affordance to facilitate international agreements
      and partnerships <d-cite key="ho2023international,trager2023governance">[46,47]</d-cite>. At the same time,
      they should
      minimize undue bureaucratic hurdles for smaller and more predictable low-risk models. The most pressing scrutiny
      is on a small number of most powerful models – trained on billion-dollar supercomputers – which will have the most
      hazardous and unpredictable capabilities <d-cite
        key="anderljung2023frontier,ganguli2022predictability">[48,49]</d-cite>.</p>
    <p>The urgent first step is to mandate transparent AI development. To regulate effectively, regulators need
      visibility into frontier AI development through model registration, whistleblower protections, incident reporting,
      and monitoring of model development and supercomputer usage <d-cite
        key="anderljung2023frontier,hadfield2023national,mitchell2019model,ainow2023euaia,incidentdatabase2023ai,bloch2023promise,mulani2023proposing">[48,50–55]</d-cite>. Regulators
      also
      need access to advanced models before deployment to evaluate them for dangerous capabilities such as autonomous
      self-replication, offensive cyber capabilities, or making pandemic pathogens widely accessible <d-cite
        key="shevlane2023model,mokander2023auditing,soice2023large">[44,56,57]</d-cite>.</p>
    <p>For models with hazardous capabilities, we need a combination of governance mechanisms <d-cite
        key="anderljung2023frontier,ainow2023euaia,schuett2023best">[48,52,58]</d-cite> matched to the magnitude and
      predictability of their risks. We ask companies to promptly lay out detailed and independently scrutinized if-then
      commitments: specific safety measures they will take if specific hazardous capabilities are found. Regulators
      should create national and international safety standards that depend on model capabilities. They should also hold
      companies accountable for harms from their models that can be reasonably foreseen and prevented. These measures
      prevent harm and also incentivize companies to invest in safety. For exceptionally capable future models, e.g.
      models that could circumvent human control, governments must be prepared to license their development, pause
      development in response to worrying capabilities, mandate access controls, and require information security
      measures robust to state-level hackers, until adequate safeguards are ready.</p>
    <p>AI may well be the technology that shapes this century. While AI capabilities are advancing rapidly, progress in
      safety and governance is lagging behind. To steer AI toward positive outcomes and away from catastrophe, we need
      to reorient. There is a responsible path forward, if we have the wisdom to take it.</p>
    <ul>
      <li><em>Alternative last sentence:</em> For once, humanity must look before it leaps.</li>
    </ul>
  </d-article>
  <d-appendix class="centered">
    <h3>Acknowledgments</h3>

    <p>This website uses <a href="https://github.com/distillpub/template">Distill</a>, licensed under the Apache License 2.0.</p>


    <h3 id="citation">Citation</h3>
        <p>Please cite this work as</p>
        <pre class="citation">Citation Here</pre>
        </pre>
    
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>
  
</body>

</html>